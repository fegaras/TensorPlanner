#!/bin/bash
#SBATCH -A uot166
#SBATCH --job-name="diablo"
#SBATCH --output="run.log"
#SBATCH --partition=compute
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=2
#SBATCH --cpus-per-task=64
#SBATCH --mem=249208M
#SBATCH --export=ALL
#SBATCH --time=30    # time limit in minutes

module purge
module load slurm cpu/0.17.3b gcc/10.2.0/npcyll4 mvapich2/2.3.7/iyjtn3x

SW=/expanse/lustre/projects/uot166/fegaras
export DIABLO_HOME=/home/$USER/TensorPlanner
export SCALA_HOME=$SW/scala-2.12.3
export SPARK_HOME=$SW/spark-3.1.2-bin-hadoop3.2

JARS=${DIABLO_HOME}/lib/diablo.jar
for I in core sql; do
    JARS=${JARS}:`ls ${SPARK_HOME}/jars/spark-${I}*.jar`
done
export JARS=${JARS}

export PATH="$SCALA_HOME/bin:$DIABLO_HOME/bin:$PATH"

file="mult.diablo"
echo compiling $file ...
diablo $file

ulimit -l unlimited
ulimit -s unlimited

export MV2_SMP_USE_CMA=0
export MV2_USE_RDMA_CM=0
export MV2_HOMOGENEOUS_CLUSTER=1
export MV2_ENABLE_AFFINITY=0
export MV2_USE_ALIGNED_ALLOC=1
export MV2_CPU_BINDING_LEVEL=socket

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

export diablo_collect=false

srun --mpi=pmi2 -n $SLURM_NTASKS -c $SLURM_CPUS_PER_TASK ./a.out 20000 10
