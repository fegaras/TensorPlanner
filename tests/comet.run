#!/bin/bash
#SBATCH -A uot166
#SBATCH --job-name="diablo"
#SBATCH --output="run.log"
#SBATCH --partition=compute
#SBATCH --nodes=4
#SBATCH --export=ALL
#SBATCH --time=600    # time limit in minutes

nodes=$SLURM_NNODES
echo "Number of nodes = " $nodes

# Comet node: 24 cores (23 available), 128 GB RAM
#   executor-cores = 11   (2 executors/node)
#   executor-memory = 60GB
#   num-executors = nodes*2-1
executors=$((nodes*2-1))
echo "Number of executors = " $executors

SPARK_OPTIONS="--driver-memory 60G --num-executors $executors --executor-cores 11 --executor-memory 60G --supervise"

export HADOOP_CONF_DIR=$HOME/cometcluster
module load hadoop

# location of hadoop, spark, scala, and diablo
SW=/oasis/projects/nsf/uot143/fegaras

export DIABLO_HOME=$SW/diablo
export JAVA_HOME=/lib/jvm/java
export SCALA_HOME=$SW/scala-2.12.3
export HADOOP_HOME=$SW/hadoop-3.2.2
export SPARK_HOME=$SW/spark-3.0.1-bin-hadoop3.2

myhadoop-configure.sh
source $HOME/cometcluster/spark/spark-env.sh
export SPARK_MASTER_HOST=$SPARK_MASTER_IP
# start HDFS
$HADOOP_HOME/sbin/start-dfs.sh
# start Spark
$SPARK_HOME/sbin/start-all.sh -h $SPARK_MASTER_HOST

JARS=.
for I in $SPARK_HOME/jars/*.jar; do
    JARS=$JARS:$I
done

rm -rf classes
mkdir -p classes

scala_files="add.scala multiply.scala factorization.scala"
for f in $scala_files; do
    echo compiling $f ...
    $SCALA_HOME/bin/scalac -d classes -cp classes:${JARS}:${DIABLO_HOME}/lib/diablo.jar $f
done

jar cf test.jar -C classes .

scale=100        # scale of datasets
ns=5             # number of datasets per experiment
repeat=4         # number of repetitions of each experiment

for ((i=1; i<=$ns; i++)); do   # for each different dataset

   j=$(echo "scale=3;sqrt($i/$ns)*400*$scale" | bc); n=${j%.*}
      $SPARK_HOME/bin/spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Add --master $MASTER $SPARK_OPTIONS test.jar $repeat $n

   j=$(echo "scale=3;sqrt($i/$ns)*80*$scale" | bc); n=${j%.*}
      $SPARK_HOME/bin/spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Multiply --master $MASTER $SPARK_OPTIONS test.jar $repeat $n

   j=$(echo "scale=3;sqrt($i/$ns)*200*$scale" | bc); n=${j%.*}
      $SPARK_HOME/bin/spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Multiply --master $MASTER $SPARK_OPTIONS test.jar $repeat $n 1

   j=$(echo "scale=3;sqrt($i/$ns)*200*$scale" | bc); n=${j%.*}
      $SPARK_HOME/bin/spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Factorization --master $MASTER $SPARK_OPTIONS test.jar $repeat $n $n 1000 0

   j=$(echo "scale=3;sqrt($i/$ns)*200*$scale" | bc); n=${j%.*}
      $SPARK_HOME/bin/spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Factorization --master $MASTER $SPARK_OPTIONS test.jar $repeat $n $n 1000 1

done

$SPARK_HOME/sbin/stop-all.sh
$HADOOP_HOME/sbin/stop-dfs.sh
myhadoop-cleanup.sh
